{
  "bigram": {
    "runtime": {
      "training": true,
      "batch_size": 8,
      "block_size": 4,
      "steps": 10000,
      "interval": 100,
      "lr": 0.001,
      "patience": 10,
      "max_new_tokens": 100,
      "max_checkpoints": 10
    },
    "model": {}
  },
  "lstm": {
    "runtime": {
      "training": true,
      "batch_size": 16,
      "block_size": 64,
      "steps": 50000,
      "interval": 500,
      "lr": 0.0015,
      "patience": 10,
      "max_new_tokens": 200,
      "max_checkpoints": 10
    },
    "model": {
      "embedding_dim": 64,
      "hidden_size": 128,
      "num_layers": 2
    }
  },
  "transformer": {
    "runtime": {
      "block_size": 32,
      "max_new_tokens": 240
    },
    "model": {}
  }
}
