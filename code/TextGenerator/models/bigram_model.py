import torch
import torch.nn as nn
from models.base_model import BaseLanguageModel
from utils import decode_data


class BigramLanguageModel(BaseLanguageModel):
    """
    A bigram language model that predicts the next character in a sequence.

    Architecture:
        - Single embedding layer that directly maps characters to their
          next character probabilities
        - No memory of previous context beyond the current character

    This is a simple baseline model that only considers the previous
    character when making predictions.
    """

    def __init__(self, vocab_size: int) -> None:
        """Initialize the bigram model and its parameters."""
        super().__init__(vocab_size, model_name="bigram")
        # Each character gets a vector of size vocab_size
        # Character predictions are learned via probability distribution
        self.embedding = nn.Embedding(vocab_size, vocab_size)

    def __repr__(self) -> str:
        """Return a string representation of the model."""
        return f"BigramLanguageModel(vocab_size={self.vocab_size})"

    def forward(
        self, idx: torch.Tensor, targets: torch.Tensor | None = None
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        """
        Compute logits and loss for input indices and targets.

        Args:
            idx: Input token indices of shape (B, T)
            targets: Target token indices of shape (B, T), optional

        Returns:
            tuple: (logits, loss) where:
                - logits: Model predictions of shape (B, T, vocab_size)
                - loss: Cross entropy loss if targets provided, None otherwise
        """
        # (B, T, vocab_size): map indices to logits for next character prediction
        logits = self.embedding(idx)
        logits, loss = self.compute_loss(idx, logits, targets)

        return logits, loss

    @torch.no_grad()
    def generate(
        self,
        start_idx: int,
        itos: dict[int, str],
        max_new_tokens: int,
    ) -> str:
        """
        Generate new text by sampling from the model's predictions.
        Uses multinomial sampling to add randomness to the output.
        Starts from a seed index and generates max_new_tokens characters.
        Returns the decoded string generated by the model.
        """
        self.eval()
        idx = torch.tensor([[start_idx]], dtype=torch.long, device=self.device)
        generated = [start_idx]

        for _ in range(max_new_tokens):
            # Get predictions for next step
            logits, _ = self(idx)
            next_idx = self.new_token(logits)
            generated.append(next_idx[0, 0].item())

        return decode_data(generated, itos)
